{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a13b6b-e5ca-4ac6-acfc-536591f99bf6",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7bf1f-0308-43ba-9075-7fa80912a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involves predicting a \n",
    "continuous numeric value as the output variable. It is a type of ensemble learning method that combines the predictions \n",
    "from multiple individual regression models to create a more accurate and robust predictive model.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "Base Learners (Weak Models):\n",
    "Gradient Boosting starts with an initial prediction, typically the mean of the target variable for all training examples.\n",
    "Then, it iteratively builds a sequence of regression models, often referred to as \"weak\" or \"base\" learners. These base\n",
    "learners are usually decision trees, but they can also be other types of regression models.\n",
    "\n",
    "Residuals:\n",
    "In each iteration, the algorithm identifies the differences (residuals) between the actual target values and the predictions\n",
    "made by the current ensemble of base learners. These residuals represent the errors of the current model.\n",
    "\n",
    "Fit a New Base Learner to Residuals:\n",
    "The next base learner is trained to predict these residuals instead of the original target values. The new base learner is\n",
    "trained to minimize the residual errors from the previous step. This step is the key to gradient boosting's effectiveness.\n",
    "It focuses on the mistakes made by the current ensemble of models.\n",
    "\n",
    "Update Ensemble:\n",
    "The predictions of all the base learners, including the new one, are combined to update the ensemble's predictions. Each base\n",
    "learner is assigned a weight in the ensemble based on its performance, and their predictions are added together to form the\n",
    "updated predictions.\n",
    "\n",
    "Iterate:\n",
    "Steps 2 to 4 are repeated for a specified number of iterations (or until a convergence criterion is met). Each iteration focuses\n",
    "on reducing the errors made by the previous ensemble, which gradually improves the overall prediction accuracy.\n",
    "\n",
    "Final Prediction:\n",
    "The final prediction is obtained by summing up the predictions of all base learners. This aggregated prediction is often very\n",
    "accurate, as the model learns to correct its errors over multiple iterations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2111267b-9929-4687-a2c3-f422f77e9388",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9850421-7201-453d-beb2-3df922cbe1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define Iris Dataset\n",
    "df=sns.load_dataset('iris')\n",
    "\n",
    "\n",
    "# Split Dataset into X and y \n",
    "X=df.drop('species',axis=1)\n",
    "y=df['species']\n",
    "\n",
    "\n",
    "# Split X,y into train and test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.20,random_state=42)\n",
    "\n",
    "\n",
    "#Encode Dependent variable\n",
    "encoder=LabelEncoder()\n",
    "y_train=encoder.fit_transform(y_train)\n",
    "y_test=encoder.transform(y_test)\n",
    "\n",
    "\n",
    "#train and define model\n",
    "clf=GradientBoostingClassifier()\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: \",accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c1c2b-6104-4df7-98dd-3cb975ec51e2",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "514d50ab-8f96-48bc-9211-62516b29855b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'n_estimators': 30, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "# import library for hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define hyperparameter grid for the grid search\n",
    "param_grid={'learning_rate':[0.01,0.1,0.2],\n",
    "            'n_estimators':[10,20,30],\n",
    "             'max_depth':[1,2,3]}\n",
    "\n",
    "# Initialize the RandomizedSearchCV\n",
    "grid=RandomizedSearchCV(GradientBoostingClassifier(),param_distributions=param_grid,cv=5,scoring='accuracy')\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "#prediction\n",
    "y_pred=grid.predict(X_test)\n",
    "\n",
    "# Best Parameters\n",
    "print(\"Best Parameters: \",grid.best_params_)\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy: \",accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d0625-013b-4c3c-8e82-97a0b657d912",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3427628-0d44-4f19-b097-38af444e99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In Gradient Boosting, a weak learner is a simple and moderately accurate model used within the ensemble framework.\n",
    "Weak learners, such as shallow decision trees or linear models, are only slightly better than random guessing.\n",
    "The essence of Gradient Boosting lies in sequentially adding and training these weak learners to improve the overall\n",
    "predictive performance.\n",
    "\n",
    "Starting with an initial prediction, subsequent learners are trained to correct the errors made by the existing ensemble.\n",
    "They focus on predicting the residuals, i.e., the differences between actual target values and ensemble predictions.\n",
    "Each learner is assigned a weight based on its contribution to error reduction, ensuring that more accurate models have \n",
    "greater influence in the ensemble.\n",
    "\n",
    "The boosting principle underscores Gradient Boosting, as each weak learner boosts the ensemble's accuracy by addressing\n",
    "its deficiencies. This iterative process, performed over multiple rounds or until convergence, incrementally refines the\n",
    "model's predictions. Though weak learners may individually lack power, their cumulative effect leads to a robust and\n",
    "accurate predictive model. Gradient Boosting has become indispensable in machine learning, excelling in various domains,\n",
    "from finance to healthcare, thanks to its ability to capture intricate data patterns and handle challenging datasets through \n",
    "the collaborative strength of these weak yet collectively powerful models.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99aeba-e5f2-402a-8267-a4d01f2cb9e2",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f10fef-f153-4455-a4b0-5e5a86148f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequential Error Correction: \n",
    "Gradient Boosting builds an ensemble model by sequentially adding weak learners (simple models) to correct the errors\n",
    "made by the existing ensemble. Each new learner focuses on the mistakes made by the previous ones, gradually improving\n",
    "the overall model.\n",
    "\n",
    "Gradient Descent:\n",
    "The \"Gradient\" in Gradient Boosting refers to the use of gradient descent optimization. Instead of trying to find the\n",
    "best-fitting model parameters directly, the algorithm minimizes the errors (residuals) by updating the model iteratively.\n",
    "It does this by moving in the direction of steepest decrease in the loss function, which represents the difference between \n",
    "predictions and actual target values.\n",
    "\n",
    "Weighted Learning:\n",
    "Weak learners are assigned weights based on their performance. Better models get higher weights, meaning they have more\n",
    "influence in the final prediction. This ensures that the ensemble focuses more on the areas where it's making errors.\n",
    "\n",
    "Complexity Handling:\n",
    "Gradient Boosting can capture complex relationships in the data by combining many weak learners, making it robust against\n",
    "overfitting and able to model nonlinear patterns effectively.\n",
    "\n",
    "High Predictive Power:\n",
    "By iteratively improving predictions, Gradient Boosting often achieves high predictive accuracy. It's particularly well-suited\n",
    "for tasks where traditional models may struggle, such as in noisy or heterogeneous datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7d774-4073-43e8-b394-e2163f2647a0",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb52a69-6f2c-4ffe-b05f-9905e35cde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner to create a strong \n",
    "predictive model.\n",
    "\n",
    "Here's a step-by-step explanation of how Gradient Boosting constructs this ensemble:\n",
    "\n",
    "Initialization:\n",
    "Gradient Boosting starts with an initial prediction for the target variable. This initial prediction is often a simple \n",
    "value, such as the mean of the target values for regression tasks or the log-odds ratio for binary classification tasks.\n",
    "\n",
    "Iterative Process:\n",
    "The algorithm performs a series of iterations, and in each iteration, it adds a new weak learner (base model) to the\n",
    "ensemble. \n",
    "Here's what happens in each iteration:\n",
    "a. Compute Residuals: The algorithm calculates the residuals, which are the differences between the actual target values \n",
    "   and the current predictions made by the ensemble. These residuals represent the errors that the current ensemble is making.\n",
    "b. Train a Weak Learner: The next weak learner is trained to predict these residuals. The weak learner's goal is to capture the\n",
    "   patterns in the data that the current ensemble is not modeling well. Common choices for weak learners include shallow decision\n",
    "   trees, linear models, or other simple algorithms.\n",
    "c. Update Ensemble's Predictions: The predictions of the newly trained weak learner are added to the current ensemble's predictions. \n",
    "   However, these predictions are weighted based on the learner's performance. Better-performing learners are given higher weights, \n",
    "   meaning they have a stronger influence on the ensemble's prediction.\n",
    "\n",
    "Repeat Iterations:\n",
    "Steps 2a to 2c are repeated for a predefined number of iterations or until a convergence criterion is met. With each iteration, the\n",
    "ensemble's predictions become more accurate as it learns to correct its errors.\n",
    "\n",
    "Final Prediction: \n",
    "The final prediction is obtained by summing up the predictions of all weak learners in the ensemble, each  weighted according\n",
    "to its performance. This aggregated prediction represents the ensemble's final output, which is typically a highly accurate and\n",
    "robust prediction for the target variable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e823bb-2017-45cc-8719-77e75713e889",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc075ab9-8bc8-4aea-be99-ea2c77072ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The mathematical intuition behind the Gradient Boosting algorithm involves a sequential process of constructing an ensemble\n",
    "of weak learners. It begins with initializing predictions and calculating the errors (residuals) between these predictions \n",
    "and actual target values. In each iteration, a new weak learner is trained to predict these residuals and correct the\n",
    "ensemble's errors. The contributions of the weak learners are weighted based on their performance, with better learners \n",
    "receiving higher influence. The algorithm optimizes an objective function, often a measure of prediction error, using gradient\n",
    "descent. It computes the gradient of this function with respect to the residuals, guiding the algorithm towards minimizing the\n",
    "error. Each weak learner is trained to approximate the negative gradient, effectively learning how to reduce the errors made\n",
    "by the ensemble. This iterative process continues for a fixed number of iterations or until a stopping criterion is met. The\n",
    "final prediction is an aggregation of all weak learners' predictions, weighted by their performance, resulting in a robust and\n",
    "accurate predictive model. Regularization techniques, such as shrinkage, are often employed to control the ensemble's learning\n",
    "rate and prevent overfitting.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
